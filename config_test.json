{
  "experiment": {
    "name": "gsm8k-grpo-test",
    "run_type": "gated",
    "seed": 42,
    "output_dir": "./outputs",
    "wandb_enabled": true,
    "wandb_project": "maslow-rl"
  },

  "model": {
    "name": "Qwen/Qwen2-0.5B-Instruct",
    "dtype": "bf16",
    "device_map": "auto"
  },

  "lora": {
    "r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
    "bias": "none",
    "task_type": "CAUSAL_LM"
  },

  "data": {
    "dataset_name": "openai/gsm8k",
    "dataset_config": "main",
    "train_size": 50,
    "eval_size": 20,
    "sample_seed": 42
  },

  "training": {
    "num_train_steps": 50,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "learning_rate": 1e-4,
    "warmup_steps": 5,
    "logging_steps": 5,
    "eval_steps": 25,
    "save_steps": 25,
    "gradient_accumulation_steps": 1
  },

  "grpo": {
    "num_generations_per_prompt": 8,
    "max_new_tokens": 256,
    "temperature": 0.7,
    "top_p": 0.9,
    "kl_coef": 0.05
  },

  "rewards": {
    "tier_a_weights": {
      "parsable_json": 0.3,
      "valid_schema": 0.4,
      "numeric_answer": 0.2,
      "json_only": 0.1
    },
    "gating": {
      "k": 3,
      "tau": 0.5,
      "beta": 1.0
    },
    "tier_c": {
      "enabled": false,
      "gamma": 0.1,
      "judge_fraction": 0.25,
      "judge_model": "gpt-4"
    }
  },

  "prompt": {
    "system_message": "You must output a JSON object and nothing else. Use keys step_1, step_2, etc. for your reasoning steps (minimum 2 steps required), and answer for the final numeric answer. Each step should show one piece of mathematical work. Do not include any text before or after the JSON.",
    "user_template": "Problem: {question}"
  },

  "eval": {
    "temperature": 0.1,
    "num_samples_to_save": 20,
    "eval_at_steps": [0, 25, 50]
  }
}
